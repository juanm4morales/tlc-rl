{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: LIBSUMO_AS_TRACI=1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "%env LIBSUMO_AS_TRACI=1\n",
    "!echo $LIBSUMO_AS_TRACI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using libsumo as traci as requested by environment variable.\n"
     ]
    }
   ],
   "source": [
    "from importlib import reload\n",
    "import sys\n",
    "import os\n",
    "\n",
    "from typing import Any\n",
    "from typing import Dict\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "sys.path.append(os.path.join(current_dir, '..', '..'))\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Traffic Signal Controller Reinforcement Learning Modules\n",
    "\n",
    "import tscRL.environments.environment\n",
    "reload(tscRL.environments.environment)\n",
    "from tscRL.environments.environment import SumoEnvironment, TrafficLight as tl\n",
    "\n",
    "import tscRL.agents.dqn_agent\n",
    "reload(tscRL.agents.dqn_agent)\n",
    "from tscRL.agents.ql_agent import QLAgent\n",
    "from tscRL.agents.dqn_agent import DQNAgent\n",
    "# Include sumo-tools directory\n",
    "if \"SUMO_HOME\" in os.environ:\n",
    "    tools = os.path.join(os.environ[\"SUMO_HOME\"], \"tools\")\n",
    "    sys.path.append(tools)\n",
    "else:\n",
    "    sys.exit(\"Please declare the environment variable 'SUMO_HOME'\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sumoCfgFile_unbalanced = os.path.abspath(os.path.join(current_dir, '../../../nets/2x2_intersection/intersection_unbalanced.sumocfg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = SumoEnvironment(\n",
    "    sumocfgFile=sumoCfgFile_unbalanced,\n",
    "    deltaTime=5,\n",
    "    yellowTime=4,\n",
    "    minGreenTime=10,\n",
    "    gui=False,\n",
    "    edges=False,\n",
    "    encodeIntervals={\"waitingTime\":20},\n",
    "    maxEncodeValue={\"waitingTime\":2500},\n",
    "    laneInfos=[\"waitingTime\"],\n",
    "    rewardFn=\"diff_cumulativeWaitingTime\",\n",
    "    fixedTL=False,\n",
    "    simTime=43800,\n",
    "    warmingTime=600,\n",
    "    sumoLog=False,\n",
    "    waitingTimeMemory=1000\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.env_checker import check_env \n",
    "check_env(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn_agent = DQNAgent(env=env, learningRate=0.01, batchSize=64, explorationFraction=0.2, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn_agent.learn(episodes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.discreteClass.get_max_encoded_value()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M=2500\n",
    "I=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tscRL.util.discrete import Discrete\n",
    "\n",
    "encoder = Discrete(I,M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_values = np.linspace(0, M+M*0.5, M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_values = [encoder.log_interval(x) for x in x_values]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(x_values, i_values)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('Interval')\n",
    "plt.title('Lane info encoding')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "reload(optuna)\n",
    "from optuna.pruners import MedianPruner\n",
    "from optuna.samplers import RandomSampler\n",
    "\n",
    "from tscRL.agents.callbacks import TrialCallback\n",
    "\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "from stable_baselines3.common.monitor import Monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tscRL.agents import callbacks\n",
    "reload(callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_HYPERPARAMS = {\n",
    "    \"env\": env\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_dqn_params(trial: optuna.Trial) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Sampler for DQN hyperparams.\n",
    "\n",
    "    :param trial:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 0.001, 0.1, log=True)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [32, 64])\n",
    "    buffer_size = trial.suggest_categorical(\"buffer_size\", [100000, 500000, 1000000])\n",
    "    exploration_fraction = trial.suggest_float(\"exploration_fraction\", 0.1, 0.6)\n",
    "    target_update_interval = trial.suggest_categorical(\"target_update_interval\", [1000, 5000, 10000])\n",
    "    layer_size = trial.suggest_categorical(\"layer\", [32, 64, 128])\n",
    "    net_arch = (layer_size, layer_size)\n",
    "\n",
    "    hyperparams = {\n",
    "        \"learningRate\": learning_rate,\n",
    "        \"batchSize\": batch_size,\n",
    "        \"bufferSize\": buffer_size,\n",
    "        \"explorationFraction\": exploration_fraction,\n",
    "        \"targetUpdateInterval\": target_update_interval,\n",
    "        \"netArch\": net_arch\n",
    "    }\n",
    "\n",
    "    return hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EVAL_EPISODES = 50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial: optuna.Trial):\n",
    "    kwargs = DEFAULT_HYPERPARAMS.copy()\n",
    "    kwargs.update(sample_dqn_params(trial))\n",
    "    kwargs[\"callback\"] = TrialCallback(trial=trial, n_eval_episodes=N_EVAL_EPISODES, min_trial_fract=0.75, rewards_window_size=6, prune=False )\n",
    "\n",
    "    dqn_agent = DQNAgent(**kwargs)\n",
    "    \n",
    "    nan_encountered = False\n",
    "    try:\n",
    "        dqn_agent.learn(episodes=N_EVAL_EPISODES)\n",
    "    except AssertionError as e:\n",
    "        # Sometimes, random hyperparams can generate NaN.\n",
    "        print(e)\n",
    "        nan_encountered = True\n",
    "\n",
    "    # Tell the optimizer that the trial failed.\n",
    "    if nan_encountered:\n",
    "        return float(\"nan\")\n",
    "    if dqn_agent.callback.is_pruned:\n",
    "        raise optuna.exceptions.TrialPruned()\n",
    "    last_crs = dqn_agent.callback.last_cumulative_rewards\n",
    "    return np.mean(last_crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_TRIALS=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pruner = MedianPruner()\n",
    "\n",
    "study = optuna.create_study(pruner=pruner, direction=\"maximize\")\n",
    "\n",
    "try:\n",
    "    study.optimize(objective, n_trials=N_TRIALS)\n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "\n",
    "print(\"Number of finished trials: \", len(study.trials))\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(\"  Value: \", trial.value)\n",
    "\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.matplotlib.plot_param_importances(study2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_dqn_params2(trial: optuna.Trial) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Sampler for DQN hyperparams.\n",
    "\n",
    "    :param trial:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [32, 64])\n",
    "    buffer_size = trial.suggest_categorical(\"buffer_size\", [100000, 500000, 1000000])\n",
    "    exploration_fraction = trial.suggest_float(\"exploration_fraction\", 0.1, 0.6)\n",
    "    target_update_interval = trial.suggest_categorical(\"target_update_interval\", [1000, 5000, 10000])\n",
    "    layer_size = trial.suggest_categorical(\"layer\", [32, 64, 128])\n",
    "    net_arch = (layer_size, layer_size)\n",
    "\n",
    "    hyperparams = {\n",
    "        \"batchSize\": batch_size,\n",
    "        \"bufferSize\": buffer_size,\n",
    "        \"explorationFraction\": exploration_fraction,\n",
    "        \"targetUpdateInterval\": target_update_interval,\n",
    "        \"netArch\": net_arch\n",
    "    }\n",
    "\n",
    "    return hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective2(trial: optuna.Trial):\n",
    "    kwargs = DEFAULT_HYPERPARAMS.copy()\n",
    "    kwargs.update(sample_dqn_params2(trial))\n",
    "    kwargs[\"learningRate\"] = 0.01\n",
    "    kwargs[\"callback\"] = TrialCallback(trial=trial, n_eval_episodes=N_EVAL_EPISODES, min_trial_fract=0.75, rewards_window_size=6, prune=False )\n",
    "\n",
    "    dqn_agent = DQNAgent(**kwargs)\n",
    "    \n",
    "    nan_encountered = False\n",
    "    try:\n",
    "        dqn_agent.learn(episodes=N_EVAL_EPISODES)\n",
    "    except AssertionError as e:\n",
    "        # Sometimes, random hyperparams can generate NaN.\n",
    "        print(e)\n",
    "        nan_encountered = True\n",
    "\n",
    "    # Tell the optimizer that the trial failed.\n",
    "    if nan_encountered:\n",
    "        return float(\"nan\")\n",
    "    if dqn_agent.callback.is_pruned:\n",
    "        raise optuna.exceptions.TrialPruned()\n",
    "    last_crs = dqn_agent.callback.last_cumulative_rewards\n",
    "    return np.mean(last_crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study2 = optuna.create_study(pruner=pruner, direction=\"maximize\")\n",
    "\n",
    "try:\n",
    "    study2.optimize(objective2, n_trials=5)\n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "\n",
    "print(\"Number of finished trials: \", len(study2.trials))\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study2.best_trial\n",
    "\n",
    "print(\"  Value: \", trial.value)\n",
    "\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%store study2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.matplotlib.plot_param_importances(study2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    study2.optimize(objective2, n_trials=5)\n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "\n",
    "print(\"Number of finished trials: \", len(study2.trials))\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study2.best_trial\n",
    "\n",
    "print(\"  Value: \", trial.value)\n",
    "\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = optuna.visualization.plot_param_importances(study2)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = optuna.visualization.plot_intermediate_values(study)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [1,2,3,4,5,6,12]:\n",
    "    print(\"Trial \" + str(i) + \". Value= \" + str(study.trials[i].value)  + \". Params: \", end=\"\")\n",
    "    print(study.trials[i].params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [0,7,8,9,10,11]:\n",
    "    print(\"Trial \" + str(i) + \". Value= \" + str(study.trials[i].value)  + \". Params: \", end=\"\")\n",
    "    print(study.trials[i].params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede observar como un exploration_fraction bajo produce una divergencia de la solución"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_dqn_params3(trial: optuna.Trial) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Sampler for DQN hyperparams.\n",
    "\n",
    "    :param trial:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [32, 64])\n",
    "    buffer_size = trial.suggest_categorical(\"buffer_size\", [500000, 1000000])\n",
    "    exploration_fraction = trial.suggest_float(\"exploration_fraction\", 0.4, 0.75)\n",
    "    target_update_interval = trial.suggest_categorical(\"target_update_interval\", [1000, 5000, 10000])\n",
    "    layer_size = trial.suggest_categorical(\"layer\", [32, 64,128])\n",
    "    net_arch = (layer_size, layer_size)\n",
    "\n",
    "    hyperparams = {\n",
    "        \"batchSize\": batch_size,\n",
    "        \"bufferSize\": buffer_size,\n",
    "        \"explorationFraction\": exploration_fraction,\n",
    "        \"targetUpdateInterval\": target_update_interval,\n",
    "        \"netArch\": net_arch\n",
    "    }\n",
    "\n",
    "    return hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective3(trial: optuna.Trial):\n",
    "    kwargs = DEFAULT_HYPERPARAMS.copy()\n",
    "    kwargs.update(sample_dqn_params3(trial))\n",
    "    kwargs[\"learningRate\"] = 0.01\n",
    "    kwargs[\"callback\"] = TrialCallback(trial=trial, n_eval_episodes=N_EVAL_EPISODES, min_trial_fract=0.9, rewards_window_size=6, prune=False, verbose=True)\n",
    "\n",
    "    dqn_agent = DQNAgent(**kwargs)\n",
    "    \n",
    "    nan_encountered = False\n",
    "    try:\n",
    "        dqn_agent.learn(episodes=N_EVAL_EPISODES)\n",
    "    except AssertionError as e:\n",
    "        # Sometimes, random hyperparams can generate NaN.\n",
    "        print(e)\n",
    "        nan_encountered = True\n",
    "\n",
    "    # Tell the optimizer that the trial failed.\n",
    "    if nan_encountered:\n",
    "        return float(\"nan\")\n",
    "    if dqn_agent.callback.is_pruned:\n",
    "        raise optuna.exceptions.TrialPruned()\n",
    "    last_crs = dqn_agent.callback.last_cumulative_rewards\n",
    "    return np.mean(last_crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pruner = MedianPruner()\n",
    "if \"study3\" not in globals():\n",
    "    study3 = optuna.create_study(pruner=pruner, direction=\"maximize\")\n",
    "try:\n",
    "    study3.optimize(objective3, n_trials=5)\n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "\n",
    "print(\"Number of finished trials: \", len(study3.trials))\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study3.best_trial\n",
    "\n",
    "print(\"  Value: \", trial.value)\n",
    "\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%store study3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    study3.optimize(objective3, n_trials=5)\n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "\n",
    "print(\"Number of finished trials: \", len(study3.trials))\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study3.best_trial\n",
    "\n",
    "print(\"  Value: \", trial.value)\n",
    "\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = optuna.visualization.plot_param_importances(study3)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = optuna.visualization.plot_slice(study3, params=[\"layer\", \"target_update_interval\", \"exploration_fraction\"])\n",
    "fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [1,8,16]:\n",
    "    print(\"Trial \" + str(i) + \". Value= \" + str(study3.trials[i].value)  + \". Params: \", end=\"\")\n",
    "    print(study3.trials[i].params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [0,2,3,4,7,9,10,11,12,13,14,15]:\n",
    "    print(\"Trial \" + str(i) + \". Value= \" + str(study3.trials[i].value)  + \". Params: \", end=\"\")\n",
    "    print(study3.trials[i].params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_study = study.trials_dataframe()\n",
    "df_study.to_csv('parametersStudy.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_study3 = study3.trials_dataframe()\n",
    "df_study3.to_csv('parametersStudy3.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_dqn_params4(trial: optuna.Trial) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Sampler for DQN hyperparams.\n",
    "\n",
    "    :param trial:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 0.001, 0.01)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [32, 64])\n",
    "    buffer_size = trial.suggest_categorical(\"buffer_size\", [500000, 1000000])\n",
    "\n",
    "    hyperparams = {\n",
    "        \"learningRate\": learning_rate,\n",
    "        \"batchSize\": batch_size,\n",
    "        \"bufferSize\": buffer_size\n",
    "    }\n",
    "\n",
    "    return hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EVAL_EPISODES_2 = 75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective4(trial: optuna.Trial):\n",
    "    kwargs = DEFAULT_HYPERPARAMS.copy()\n",
    "    kwargs.update(sample_dqn_params4(trial))\n",
    "    kwargs[\"explorationFraction\"] = 0.5\n",
    "    kwargs[\"targetUpdateInterval\"] = 1000\n",
    "    kwargs[\"netArch\"] = (32, 32)\n",
    "    kwargs[\"callback\"] = TrialCallback(trial=trial, n_eval_episodes=N_EVAL_EPISODES_2, min_trial_fract=0.9, rewards_window_size=10, prune=False, verbose=True)\n",
    "\n",
    "    dqn_agent = DQNAgent(**kwargs)\n",
    "    \n",
    "    nan_encountered = False\n",
    "    try:\n",
    "        dqn_agent.learn(episodes=N_EVAL_EPISODES_2)\n",
    "    except AssertionError as e:\n",
    "        # Sometimes, random hyperparams can generate NaN.\n",
    "        print(e)\n",
    "        nan_encountered = True\n",
    "\n",
    "    # Tell the optimizer that the trial failed.\n",
    "    if nan_encountered:\n",
    "        return float(\"nan\")\n",
    "    if dqn_agent.callback.is_pruned:\n",
    "        raise optuna.exceptions.TrialPruned()\n",
    "    last_crs = dqn_agent.callback.last_cumulative_rewards\n",
    "    return np.mean(last_crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pruner = MedianPruner()\n",
    "\n",
    "study4 = optuna.create_study(pruner=pruner, direction=\"maximize\")\n",
    "for _ in range(5):\n",
    "    study4.optimize(objective4, n_trials=1)\n",
    "    %store study4\n",
    "\n",
    "\n",
    "print(\"Number of finished trials: \", len(study4.trials))\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study4.best_trial\n",
    "\n",
    "print(\"  Value: \", trial.value)\n",
    "\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(5):\n",
    "    study4.optimize(objective4, n_trials=1)\n",
    "    %store study4\n",
    "\n",
    "\n",
    "print(\"Number of finished trials: \", len(study4.trials))\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study4.best_trial\n",
    "\n",
    "print(\"  Value: \", trial.value)\n",
    "\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_study4 = study4.trials_dataframe()\n",
    "df_study4.to_csv('parametersStudy4.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = optuna.visualization.plot_param_importances(study4)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = optuna.visualization.plot_slice(study4)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = optuna.visualization.plot_intermediate_values(study4)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = SumoEnvironment(\n",
    "    sumocfgFile=sumoCfgFile_unbalanced,\n",
    "    deltaTime=5,\n",
    "    yellowTime=4,\n",
    "    minGreenTime=10,\n",
    "    gui=False,\n",
    "    edges=False,\n",
    "    discreteIntervals=I,\n",
    "    maxLaneValue=M, \n",
    "    laneInfo=\"waitingTime\",\n",
    "    rewardFn=\"diff_cumulativeWaitingTime\",\n",
    "    fixedTL=False,\n",
    "    simTime=43800, \n",
    "    sumoLog=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn_agent = DQNAgent(\n",
    "    env=env,\n",
    "    learningRate=0.001,\n",
    "    batchSize=64,\n",
    "    explorationFraction=0.5,\n",
    "    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dqn_agent.model.get_env())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID = \"dqn_unbalanced\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn_agent.learn(100)\n",
    "dqn_agent.model.save(MODEL_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = SumoEnvironment(\n",
    "    sumocfgFile=sumoCfgFile_unbalanced,\n",
    "    deltaTime=5,\n",
    "    yellowTime=4,\n",
    "    minGreenTime=10,\n",
    "    gui=True,\n",
    "    edges=False,\n",
    "    discreteIntervals=I,\n",
    "    maxLaneValue=M, \n",
    "    laneInfo=\"waitingTime\",\n",
    "    rewardFn=\"diff_cumulativeWaitingTime\",\n",
    "    fixedTL=False,\n",
    "    simTime=43800, \n",
    "    sumoLog=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn_agent_ = DQNAgent(\n",
    "    env=env,\n",
    "    learningRate=0.001,\n",
    "    batchSize=64,\n",
    "    explorationFraction=0.5,\n",
    "    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelFileLocation = os.path.join(os.getcwd(), MODEL_ID)\n",
    "modelFileLocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn_agent_.loadModel(modelFileLocation, env)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn_agent_.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_dqn_unbalanced = dqn_agent.callback.get_metrics()\n",
    "df_dqn_unbalanced = pd.DataFrame(metrics_dqn_unbalanced)\n",
    "df_dqn_unbalanced.to_csv('df_dqn_unbalanced.csv', index=False) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def serializeRuns(runs, fileName):\n",
    "    rows = []\n",
    "    # Iterar sobre la lista de diccionarios\n",
    "    for run, entry in enumerate(runs):\n",
    "        for episode, mwt, mawt, cr, t in zip(entry[\"episode\"], entry[\"mean_waiting_time\"], entry[\"mean_acc_waiting_time\"], entry[\"cumulative_reward\"], entry[\"time\"]):\n",
    "            rows.append({\"run\": run, \"episode\": episode, \"mean_waiting_time\": mwt, \"mean_acc_waiting_time\": mawt, \"cumulative_reward\": cr, \"time\": t})\n",
    "\n",
    "    # Crear el DataFrame\n",
    "    df = pd.DataFrame(rows)\n",
    "    df.to_csv(fileName, index=False)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_balanced_unb_ftl = pd.read_csv(\"df_unbalanced_ftl.csv\")\n",
    "df_dqn_unbalanced = pd.read_csv(\"df_dqn_unbalanced.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mean cumulative waiting time in fixed TLC: \" + str(df_balanced_unb_ftl[\"mean_acc_waiting_time\"].mean()))\n",
    "print(\"Mean cumulative waiting time in final trained DQNTLC: \" + str(df_dqn_unbalanced.loc[df_dqn_unbalanced['episode'] == 100, 'mean_acc_waiting_time'].values[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "plt.plot(df_dqn_unbalanced[\"episode\"], df_dqn_unbalanced['mean_acc_waiting_time'], label=\"DQN Agent\")\n",
    "\n",
    "plt.plot(df_balanced_unb_ftl[\"episode\"], df_balanced_unb_ftl['mean_acc_waiting_time'], label=\"Fixed TL\")\n",
    "\n",
    "plt.xlabel('Episode', fontsize=20)\n",
    "plt.ylabel('Mean Waiting Time (in seconds)')\n",
    "plt.title('DQN Agent - Mean Accumulated Waiting Time per Episode')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.plot(df_dqn_unbalanced[\"episode\"], df_dqn_unbalanced['cumulative_reward'])\n",
    "\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Cumulative Reward')\n",
    "plt.title('DQN Agent - Cumulative Reward per Episode')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.plot(df_dqn_unbalanced[\"episode\"], df_dqn_unbalanced['loss_value'])\n",
    "\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Loss value')\n",
    "plt.title('DQN Agent - Loss per Episode')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sumoCfgFile_balanced = os.path.abspath(os.path.join(current_dir, '../../nets/2x2_intersection/intersection_balanced.sumocfg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_balanced = SumoEnvironment(\n",
    "    sumocfgFile=sumoCfgFile_balanced,\n",
    "    deltaTime=5,\n",
    "    yellowTime=4,\n",
    "    minGreenTime=10,\n",
    "    gui=False,\n",
    "    edges=False,\n",
    "    discreteIntervals=I,\n",
    "    maxLaneValue=M, \n",
    "    laneInfo=\"waitingTime\",\n",
    "    rewardFn=\"diff_cumulativeWaitingTime\",\n",
    "    fixedTL=False,\n",
    "    simTime=43800, \n",
    "    sumoLog=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn_agent = DQNAgent(\n",
    "    env=env_balanced,\n",
    "    learningRate=0.001,\n",
    "    batchSize=64,\n",
    "    explorationFraction=0.5,\n",
    "    verbose=1\n",
    ")\n",
    "dqn_agent.learn(episodes=100)\n",
    "metrics = dqn_agent.callback.get_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_balanced = pd.DataFrame(metrics)\n",
    "df_balanced.to_csv(\"dqn_agent_balanced\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tscRL.agents import fixedTL_agent as ftl_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ftl_agent.FixedTLAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sumoCfgFile_balanced = os.path.abspath(os.path.join(current_dir, '../../nets/2x2_intersection/intersection_balanced.sumocfg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_unbalanced_ftl = SumoEnvironment(\n",
    "    sumocfgFile=sumoCfgFile_unbalanced,\n",
    "    deltaTime=5,\n",
    "    yellowTime=4,\n",
    "    minGreenTime=10,\n",
    "    gui=False,\n",
    "    edges=False,\n",
    "    discreteIntervals=I,\n",
    "    maxLaneValue=M, \n",
    "    laneInfo=\"waitingTime\",\n",
    "    rewardFn=\"diff_cumulativeWaitingTime\",\n",
    "    fixedTL=True,\n",
    "    simTime=43800, \n",
    "    sumoLog=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_balanced_ftl = SumoEnvironment(\n",
    "    sumocfgFile=sumoCfgFile_balanced,\n",
    "    deltaTime=5,\n",
    "    yellowTime=4,\n",
    "    minGreenTime=10,\n",
    "    gui=False,\n",
    "    edges=False,\n",
    "    discreteIntervals=I,\n",
    "    maxLaneValue=M, \n",
    "    laneInfo=\"waitingTime\",\n",
    "    rewardFn=\"diff_cumulativeWaitingTime\",\n",
    "    fixedTL=True,\n",
    "    simTime=43800, \n",
    "    sumoLog=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ftlAgent = ftl_agent.FixedTLAgent(environment=env_unbalanced_ftl, episodes=100)\n",
    "metrics_unb_ftl = ftlAgent.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ftlAgent = ftl_agent.FixedTLAgent(environment=env_balanced_ftl, episodes=100)\n",
    "metrics_bal_ftl = ftlAgent.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unbalanced_ftl = pd.DataFrame(metrics_unb_ftl)\n",
    "df_unbalanced_ftl.to_csv('df_unbalanced_ftl.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "float(df_balanced_ftl[\"mean_acc_waiting_time\"].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "float(df_unbalanced_ftl[\"mean_acc_waiting_time\"].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "float(df_balanced_ftl[\"mean_acc_waiting_time\"].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unbalanced_ftl=pd.read_csv(\"df_unbalanced_ftl.csv\")\n",
    "df_balanced_ftl=pd.read_csv(\"df_balanced_ftl.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_balanced_ftl = pd.DataFrame(metrics_bal_ftl)\n",
    "df_balanced_ftl.to_csv('df_balanced_ftl.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_balanced_ftl[\"mean_acc_waiting_time\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dqn_balanced[df_dqn_balanced['episode'] == 99][\"mean_acc_waiting_time\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(df_dqn_balanced[\"episode\"], df_dqn_balanced['mean_acc_waiting_time'], label=\"DQN Agent\")\n",
    "plt.plot(df_balanced_ftl[\"episode\"], df_balanced_ftl['mean_acc_waiting_time'], label=\"Fixed TL\")\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Mean Waiting Time (in seconds)')\n",
    "plt.title('DQN Agent - Mean Accumulated Waiting Time per Episode')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(df_balanced[\"episode\"], df_balanced['cumulative_reward'])\n",
    "\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Cumulative Reward')\n",
    "plt.title('DQN Agent - Cumulative Reward per Episode')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
